{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3 - Basic Artificial Neural Network\n",
    "\n",
    "In this lab we will build a very rudimentary Artificial Neural Network (ANN) and use it to solve some basic classification problems. This example is implemented with only basic math and linear algebra functions using Python's scientific computing library [numpy](http://www.numpy.org/). This will allow us to study how each aspect of the network works, and to gain an intuitive understanding of its functions. In future labs we will use higher-level libraries such as Keras and Tensorflow which automate and optimize most of these functions, making the network much faster and easier to use.\n",
    "\n",
    "The code and MNIST test data is taken directly from [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/) by [Michael Nielsen](http://michaelnielsen.org/). Please review the [first chapter](http://neuralnetworksanddeeplearning.com/chap1.html) of the book for a thorough explanation of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the Python libraries we will be using, including the random library for generating random numbers, numpy for scientific computing, matplotlib and seaborn for creating data visualizations, and several helpful modules from the sci-kit learn machine learning library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will build the artificial neural network by defining a new class called `Network`. This class will contain all the data for our neural network, as well as all the methods we need to compute activations between each layer, and train the network through backpropagation and stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs for later layers.\"\"\"\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedforward (self, a):\n",
    "        \n",
    "        \"\"\"Return the output of the network if \"a\" is input. The np.dot() \n",
    "        function computes the matrix multiplication between the weight and input\n",
    "        matrices for each set of layers. When used with numpy arrays, the '+'\n",
    "        operator performs matrix addition.\"\"\"\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The \"training_data\" is a list of tuples\n",
    "        \"(x, y)\" representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters specify the number \n",
    "        of epochs, size of each mini-batch, and the learning rate.  \n",
    "        If \"test_data\" is provided then the network will be evaluated \n",
    "        against the test data after each epoch, and partial progress \n",
    "        printed out.  This is useful for tracking progress, but slows\n",
    "        things down substantially.\"\"\"\n",
    "        \n",
    "        # create an empty array to store the accuracy results from each epoch\n",
    "        results = []\n",
    "\n",
    "        n = len(training_data)\n",
    "        \n",
    "        if test_data: \n",
    "            n_test = len(test_data)\n",
    "            \n",
    "        # this is the code for one training step, done once for each epoch\n",
    "        for j in xrange(epochs):\n",
    "            \n",
    "            # before each epoch, the data is randomly shuffled\n",
    "            random.shuffle(training_data)\n",
    "            \n",
    "            # training data is broken up into individual mini-batches\n",
    "            mini_batches = [ training_data[k:k+mini_batch_size] \n",
    "                            for k in xrange(0, n, mini_batch_size) ]\n",
    "            \n",
    "            # then each mini-batch is used to update the parameters of the \n",
    "            # network using backpropagation and the specified learning rate\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            # if a test data set is provided, the accuracy results \n",
    "            # are displayed and stored in the 'results' array\n",
    "            if test_data:\n",
    "                num_correct = self.evaluate(test_data)\n",
    "                accuracy = \"%.2f\" % (100 * (float(num_correct) / n_test))\n",
    "                print \"Epoch\", j, \":\", num_correct, \"/\", n_test, \"-\", accuracy, \"% acc\"\n",
    "                results.append(accuracy)\n",
    "            else:\n",
    "                print \"Epoch\", j, \"complete\"\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
    "        is the learning rate.\"\"\"\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        \n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        \"\"\"Note that the variable l in the loop below is used a little\n",
    "        differently to the notation in Chapter 2 of the book.  Here,\n",
    "        l = 1 means the last layer of neurons, l = 2 is the\n",
    "        second-last layer, and so on.  It's a renumbering of the\n",
    "        scheme in the book, used here to take advantage of the fact\n",
    "        that Python can use negative indices in lists.\"\"\"\n",
    "        \n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\n",
    "        \n",
    "        Numpy's argmax() function returns the position of the \n",
    "        largest element in an array. We first create a list of \n",
    "        predicted value and target value pairs, and then count \n",
    "        the number of times those values match to get the total \n",
    "        number correct.\"\"\"\n",
    "        \n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        \n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define two helper functions which compute the sigmoid activation function and it's derivative which is used in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "#     The sigmoid activation function.\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "#     Derivative of the sigmoid function.\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris dataset example\n",
    "\n",
    "Now we will test our basic artificial neural network on a very simple classification problem. First we will use the [seaborn data visualization library](https://stanford.edu/~mwaskom/software/seaborn/) to load the ['iris' dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), \n",
    "which consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor), with four features measuring the length and the width of each flower's sepals and petals. After we load the data we will vizualize it using a pairwise plot using a buit-in function in seaborn. A pairwise plot is a kind of exploratory data analysis that helps us to find relationships between pairs of features within a multi-dimensional data set. In this case, we can use it to understand which features might be most useful for determining the species of the flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data = sns.load_dataset(\"iris\")\n",
    "\n",
    "# randomly shuffle data\n",
    "iris_data = shuffle(iris_data)\n",
    "\n",
    "# print first 5 data points\n",
    "print iris_data[:5]\n",
    "\n",
    "# create pairplot of iris data\n",
    "g = sns.pairplot(iris_data, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will prepare the data set for training in our ANN. Here is a list of operations we need to perform on the data set so that it will work with the `Network class` we created above:\n",
    "\n",
    "1. Convert data to numpy format\n",
    "2. Normalize the data so that each features is scaled from 0 to 1\n",
    "2. Split data into feature and target data sets by extracting specific rows from the numpy array. In this case the features are in the first four columns, and the target is in the last column, which in Python we can access with a negative index\n",
    "3. Recombine the data into a single Python array, so that each entry in the array represents one sample, and each sample is composed of two numpy arrays, one for the feature data, and one for the target\n",
    "4. Split this data set into training and testing sets\n",
    "\n",
    "Finally, we also need to convert the targets of the training set to 'one-hot' encoding (OHE). OHE takes each piece of categorical data and converts it to a list of binary values the length of which is equal to the number of categories, and the position of the current category denoted with a '1' and '0' for all others. For example, in our dataset we have 3 possible categories: versicolor, virginica, and setosa. After applying OHE, versicolor becomes [1,0,0], virginica becomes [0,1,0], and setosa becomes [0,0,1]. OHE is often used to represent target data in neural networks because it allows easy comparison to the output coming from the network's final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert iris data to numpy format\n",
    "iris_array = iris_data.as_matrix()\n",
    "\n",
    "# split data into feature and target sets\n",
    "X = iris_array[:, :4].astype(float)\n",
    "y = iris_array[:, -1]\n",
    "\n",
    "# normalize the data per feature by dividing by the maximum value in each column\n",
    "X = X / X.max(axis=0)\n",
    "\n",
    "# convert the textual category data to integer using numpy's unique() function\n",
    "_, y = np.unique(y, return_inverse=True)\n",
    "\n",
    "# convert the list of targets to a vertical matrix with the dimensions [1 x number of samples]\n",
    "# this is necessary for later computation\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "# combine feature and target data into a new python array\n",
    "data = []\n",
    "for i in range(X.shape[0]):\n",
    "    data.append(tuple([X[i].reshape(-1,1), y[i][0]]))\n",
    "\n",
    "# split data into training and test sets\n",
    "trainingSplit = int(.7 * len(data))\n",
    "training_data = data[:trainingSplit]\n",
    "test_data = data[trainingSplit:]\n",
    "\n",
    "# create an instance of the one-hot encoding function from the sci-kit learn library\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "# use the function to figure out how many categories exist in the data\n",
    "enc.fit(y)\n",
    "\n",
    "# convert only the target data in the training set to one-hot encoding\n",
    "training_data = [[_x, enc.transform(_y.reshape(-1,1)).toarray().reshape(-1,1)] for _x, _y in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the network\n",
    "net = Network([4, 32, 3])\n",
    "\n",
    "# train the network using SGD, and output the results\n",
    "results = net.SGD(training_data, 30, 10, 0.2, test_data=test_data)\n",
    "\n",
    "# visualize the results\n",
    "plt.plot(results)\n",
    "plt.ylabel('accuracy (%)')\n",
    "plt.ylim([0,100.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset example\n",
    "\n",
    "Next, we will test our ANN on another, slightly more difficult classification problem. The data set we'll be using is called MNIST, which contains tens of thousands of scanned images of handwritten digits, classified according to the digit type from 0-9. The name MNIST comes from the fact that it is a Modified (M) version of a dataset originally developed by the United States' National Institute of Standards and Technology (NIST). This is a very popular dataset used to measure the effectiveness of Machine Learning models for image recongnition. This time we don't have to do as much data management since the data is already provided in the right format [here](https://github.com/mnielsen/neural-networks-and-deep-learning/tree/master/data). \n",
    "\n",
    "We will get into more details about working with images and proper data formats for image data in later labs, but you can already use this data to test the effectiveness of our network. With the default settings you should be able to get a classification accuracy of 95% in the test set.\n",
    "\n",
    "*note: since this is a much larger data set than the Iris data, the training will take substantially more time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the matplotlib library to visualize one of the training images. In the data set, the pixel values of each 28x28 pixel image is encoded in a straight list of 784 numbers, so before we visualize it we have to use numpy's reshape function to convert it back to a 2d matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = training_data[0][0][:,0].reshape((28,28))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(img, interpolation='nearest', vmin = 0, vmax = 1, cmap=plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])\n",
    "results = net.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n",
    "\n",
    "plt.plot(results)\n",
    "plt.ylabel('accuracy (%)')\n",
    "plt.ylim([0,100.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - classification\n",
    "\n",
    "Now that you have a basic understanding of how an artificial neural network works and have seen it applied to a classification task using two types of data, see if you can use the network to solve another classification problem using another data set. \n",
    "\n",
    "In the week-3 folder there is a data set called `wine.csv` which is another common data set used to test classification capabilities of machine learning algorithms. You can find a description of the data set here:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine\n",
    "\n",
    "The code below uses numpy to import this `.csv` file as a 2d numpy array. As before, we first shuffle the data set, and then split it into feature and target sets. This time, the target is in the first column of the data, with the rest of the columns representing the 13 features. \n",
    "\n",
    "From there you should be able to go through and format the data set in a similar way as we did for the Iris data above. Remember to split the data into both training and test sets, and encode the training targets as one-hot vectors. When you create the network, make sure to specify the proper dimensions for the input and output layer so that it matches the number of features and target categories in the data set. You can also experiment with different sizes for the hidden layer. If you are not achieving good results, try changing some of the hyper-parameters, including the size and quantity of hidden layers in the network specification, and the number of epochs, the size of a mini-batch, and the learning rate in the SGD function call. With a training/test split of 80/20 you should be able to achieve 100% accuracy Within 30 epochs.\n",
    "\n",
    "Remeber to commit your changes and submit a pull request when you are done.\n",
    "\n",
    "*Hint: do not be fooled by the category labels that come with this data set! Even though the labels are already integers (1,2,3) we need to always make sure that our category labels are sequential integers __and__ start with 0. To make sure this is the case you should always use the np.unique() function on the target data as we did with the Iris example above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine_data = np.loadtxt(open(\"./data/wine.csv\",\"rb\"),delimiter=\",\")\n",
    "\n",
    "wine_data = shuffle(wine_data)\n",
    "\n",
    "X = wine_data[:,1:]\n",
    "y = wine_data[:, 0]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

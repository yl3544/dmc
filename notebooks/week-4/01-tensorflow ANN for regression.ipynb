{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4 - Tensorflow ANN for regression\n",
    "\n",
    "In this lab we will use Tensorflow to build an Artificial Neuron Network (ANN) for a regression task.\n",
    "\n",
    "As opposed to the low-level implementation from the previous week, here we will use Tensorflow to automate many of the computation tasks in the neural network. [Tensorflow](https://www.tensorflow.org/) is a higher-level open-source machine learning library [released by Google last year](https://googleblog.blogspot.com/2015/11/tensorflow-smarter-machine-learning-for.html) which is made specifically to optimize and speed up the development and training of neural networks. \n",
    "\n",
    "At its core, Tensorflow is very similar to numpy and other numerical computation libraries. Like numpy, it's main function is to do very fast computation on multi-dimensional datasets (such as computing the dot product between a vector of input values and a matrix of values representing the weights in a fully connected network). While numpy refers to such multi-dimensional data sets as 'arrays', Tensorflow calls them 'tensors', but fundamentally they are the same thing. The two main advantages of Tensorflow over custom low-level solutions are:\n",
    "\n",
    "- While it has a Python interface, much of the low-level computation is implemented in C/C++, making it run much faster than a native Python solution.\n",
    "- Many common aspects of neural networks such as computation of various losses and a variety of modern optimization techniques are implemented as built in methods, reducing their implementation to a single line of code. This also helps in development and testing of various solutions, as you can easily swap in and try various solutions without having to write all the code by hand.\n",
    "\n",
    "You can get more details about various popular machine learning libraries in [this comparison](http://deeplearning4j.org/compare-dl4j-torch7-pylearn.html).\n",
    "\n",
    "To test our basic network, we will use the [Boston Housing Dataset](https://archive.ics.uci.edu/ml/datasets/Housing), which represents data on 506 houses in Boston across 14 different features. One of the features is the median value of the house in $1000â€™s. This is a common data set for testing regression performance of machine learning algorithms. All 14 features are continuous values, making them easy to plug directly into a neural network (after normalizing ofcourse!). The common goal is to predict the median house value using the other columns as features.\n",
    "\n",
    "This lab will conclude with two assignments:\n",
    "\n",
    "- Assignment 1 (at bottom of this notebook) asks you to experiment with various regularization parameters to reduce overfitting and improve the results of the model.\n",
    "- Assignment 2 (in the next notebook) asks you to take our regression problem and convert it to a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some of the libraries we will use for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the Boston housing prices dataset. This is included with the scikit-learn library, so we can import it directly from there. The data will come in as two numpy arrays, one with all the features, and one with the target (price). We will use pandas to convert this data to a DataFrame so we can visualize it. We will then print the first 5 entries of the dataset to see the kind of data we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data from scikit-learn library\n",
    "dataset = load_boston()\n",
    "\n",
    "#load data as DataFrame\n",
    "houses = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "#add target data to DataFrame\n",
    "houses['target'] = dataset.target\n",
    "\n",
    "#print first 5 entries of data\n",
    "print houses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the dataset contains only continuous features, which we can feed directly into the neural network for training. The target is also a continuous variable, so we can use regression to try to predict the exact value of the target. You can see more information about this dataset by printing the 'DESCR' object stored in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dataset['DESCR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will do some exploratory data visualization to get a general sense of the data and how the different features are related to each other and to the target we will try to predict. First, let's plot the correlations between each feature. Larger positive or negative correlation values indicate that the two features are related (large positive or negative correlation), while values closer to zero indicate that the features are not related (no correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a datset of correlations between house features\n",
    "corrmat = houses.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "# Draw the heatmap using seaborn\n",
    "sns.set_context(\"notebook\", font_scale=0.7, rc={\"lines.linewidth\": 1.5})\n",
    "sns.heatmap(corrmat, annot=True, square=True)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a more detailed picture of the relationship between any two variables in the dataset by using seaborn's `jointplot` function and passing it two features of our data. This will show a single-dimension histogram distribution for each feature, as well as a two-dimension density scatter plot for how the two features are related. From the correlation matrix above, we can see that the `RM` feature has a strong positive correlation to the target, while the `LSTAT` feature has a strong negative correlation to the target. Let's create `jointplots` for both sets of features to see how they relate in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(houses['target'], houses['RM'], kind='hex')\n",
    "sns.jointplot(houses['target'], houses['LSTAT'], kind='hex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the plots show a positive relationship between the `RM` feature and the target, and a negative relationship between the `LSTAT` feature and the target.\n",
    "\n",
    "This type of exploratory visualization is not strictly necessary for using machine learning, but it does help to formulate your solution, and to troubleshoot your implementation incase you are not getting the results you want. For example, if you find that two features have a strong correlation with each other, you might want to include only one of them to speed up the training process. Similarly, you may want to exclude features that show little correlation to the target, since they have little influence over its value.\n",
    "\n",
    "Now that we know a little bit about the data, let's prepare it for training with our neural network. We will follow a process similar to the previous lab:\n",
    "\n",
    "- We will first re-split the data into a feature set (X) and a target set (y)\n",
    "- Then we will normalize the feature set so that the values range from 0 to 1\n",
    "- Finally, we will split both data sets into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert housing data to numpy format\n",
    "houses_array = houses.as_matrix().astype(float)\n",
    "\n",
    "# split data into feature and target sets\n",
    "X = houses_array[:, :-1]\n",
    "y = houses_array[:, -1]\n",
    "\n",
    "# normalize the data per feature by dividing by the maximum value in each column\n",
    "X = X / X.max(axis=0)\n",
    "\n",
    "# split data into training and test sets\n",
    "trainingSplit = int(.7 * houses_array.shape[0])\n",
    "\n",
    "X_train = X[:trainingSplit]\n",
    "y_train = y[:trainingSplit]\n",
    "X_test = X[trainingSplit:]\n",
    "y_test = y[trainingSplit:]\n",
    "\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up some variables that we will use to define our model. The first group are helper variables taken from the dataset which specify the number of samples in our training set, the number of features, and the number of outputs. The second group are the actual hyper-parameters which define how the model is structured and how it performs. In this case we will be building a neural network with two hidden layers, and the size of each hidden layer is controlled by a hyper-parameter. The other hyper-parameters include:\n",
    "\n",
    "- `batch size`, which sets how many training samples are used at a time\n",
    "- `learning rate` which controls how quickly the gradient descent algorithm works\n",
    "- `training epochs` which sets how many rounds of training occurs\n",
    "- `dropout keep probability`, a regularization technique which controls how many neurons are 'dropped' randomly during each training step (note in Tensorflow this is specified as the 'keep probability' from 0 to 1, with 0 representing all neurons dropped, and 1 representing all neurons kept). You can read more about dropout [here](http://neuralnetworksanddeeplearning.com/chap3.html#other_techniques_for_regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper variables\n",
    "num_samples = X_train.shape[0]\n",
    "num_features = X_train.shape[1]\n",
    "num_outputs = 1\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 50\n",
    "num_hidden_1 = 16\n",
    "num_hidden_2 = 16\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 200\n",
    "dropout_keep_prob = 1.0 # set to no dropout by default\n",
    "\n",
    "# variable to control the resolution at which the training results are stored\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a few helper functions which will dictate how error will be measured for our model, and how the weights and biases should be defined.\n",
    "\n",
    "The `accuracy()` function defines how we want to measure error in a regression problem. The function will take in two lists of values - `predictions` which represent predicted values, and `targets` which represent actual target values. In this case we simply compute the absolute difference between the two (the error) and return the average error using numpy's `mean()` fucntion.\n",
    "\n",
    "The `weight_variable()` and `bias_variable()` functions help create parameter variables for our neural network model, formatted in the proper type for Tensorflow. Both functions take in a shape parameter and return a variable of that shape using the specified initialization. In this case we are using a 'truncated normal' distribution for the weights, and a constant value for the bias. For more information about various ways to initialize parameters in Tensorflow you can consult the [documentation](https://www.tensorflow.org/versions/r0.11/api_docs/python/constant_op.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    error = np.absolute(predictions.reshape(-1) - targets)\n",
    "    return np.mean(error)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build our neural network model in Tensorflow.\n",
    "\n",
    "Tensorflow operates in a slightly different way than the procedural logic we have been using in Python so far. Instead of telling Tensorflow the exact operations to run line by line, we build the entire neural network within a structure called a `Graph`. The Graph does several things:\n",
    "\n",
    "- describes the architecture of the network, including how many layers it has and how many neurons are in each layer\n",
    "- initializes all the parameters of the network\n",
    "- describes the 'forward' calculation of the network, or how input data is passed through the network layer by layer until it reaches the result\n",
    "- defines the loss function which describes how well the model is performing\n",
    "- specifies the optimization function which dictates how the parameters are tuned in order to minimize the loss\n",
    "\n",
    "Once this graph is defined, we can work with it by 'executing' it on sets of training data and 'calling' different parts of the graph to get back results. Every time the graph is executed, Tensorflow will only do the minimum calculations necessary to generate the requested results. This makes Tensorflow very efficient, and allows us to structure very complex models while only testing and using certain portions at a time. In programming language theory, this type of programming is called ['lazy evaluation'](https://en.wikipedia.org/wiki/Lazy_evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''First we create a variable to store our graph'''\n",
    "graph = tf.Graph()\n",
    "\n",
    "'''Next we build our neural network within this graph variable'''\n",
    "with graph.as_default():\n",
    "    \n",
    "    '''Our training data will come in as x feature data and \n",
    "    y target data. We need to create tensorflow placeholders \n",
    "    to capture this data as it comes in'''\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=(None, num_features))\n",
    "    _y = tf.placeholder(tf.float32, shape=(None))\n",
    "    \n",
    "    '''Another placeholder stores the hyperparameter \n",
    "    that controls dropout'''\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    '''Finally, we convert the test and train feature data sets \n",
    "    to tensorflow constants so we can use them to generate \n",
    "    predictions on both data sets'''\n",
    "    \n",
    "    tf_X_test = tf.constant(X_test, dtype=tf.float32)\n",
    "    tf_X_train = tf.constant(X_train, dtype=tf.float32)\n",
    "    \n",
    "    '''Next we create the parameter variables for the model.\n",
    "    Each layer of the neural network needs it's own weight \n",
    "    and bias variables which will be tuned during training.\n",
    "    The sizes of the parameter variables are determined by \n",
    "    the number of neurons in each layer.'''\n",
    "    \n",
    "    W_fc1 = weight_variable([num_features, num_hidden_1])\n",
    "    b_fc1 = bias_variable([num_hidden_1])\n",
    "    \n",
    "    W_fc2 = weight_variable([num_hidden_1, num_hidden_2])\n",
    "    b_fc2 = bias_variable([num_hidden_2])\n",
    "    \n",
    "    W_fc3 = weight_variable([num_hidden_2, num_outputs])\n",
    "    b_fc3 = bias_variable([num_outputs])\n",
    "    \n",
    "    '''Next, we define the forward computation of the model.\n",
    "    We do this by defining a function model() which takes in \n",
    "    a set of input data, and performs computations through \n",
    "    the network until it generates the output.'''\n",
    "    \n",
    "    def model(data, keep):\n",
    "        \n",
    "        # computing first hidden layer from input, using relu activation function\n",
    "        fc1 = tf.nn.sigmoid(tf.matmul(data, W_fc1) + b_fc1)\n",
    "        # adding dropout to first hidden layer\n",
    "        fc1_drop = tf.nn.dropout(fc1, keep)\n",
    "        \n",
    "        # computing second hidden layer from first hidden layer, using relu activation function\n",
    "        fc2 = tf.nn.sigmoid(tf.matmul(fc1_drop, W_fc2) + b_fc2)\n",
    "        # adding dropout to second hidden layer\n",
    "        fc2_drop = tf.nn.dropout(fc2, keep)\n",
    "        \n",
    "        # computing output layer from second hidden layer\n",
    "        # the output is a single neuron which is directly interpreted as the prediction of the target value\n",
    "        fc3 = tf.matmul(fc2_drop, W_fc3) + b_fc3\n",
    "        \n",
    "        # the output is returned from the function\n",
    "        return fc3\n",
    "    \n",
    "    '''Next we define a few calls to the model() function which \n",
    "    will return predictions for the current batch input data (x),\n",
    "    as well as the entire test and train feature set'''\n",
    "    \n",
    "    prediction = model(x, keep_prob)\n",
    "    test_prediction = model(tf_X_test, 1.0)\n",
    "    train_prediction = model(tf_X_train, 1.0)\n",
    "    \n",
    "    '''Finally, we define the loss and optimization functions \n",
    "    which control how the model is trained.\n",
    "    \n",
    "    For the loss we will use the basic mean square error (MSE) function,\n",
    "    which tries to minimize the MSE between the predicted values and the \n",
    "    real values (_y) of the input dataset.\n",
    "    \n",
    "    For the optimization function we will use basic Gradient Descent (SGD)\n",
    "    which will minimize the loss using the specified learning rate.'''\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.square(tf.sub(prediction, _y)))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    '''We also create a saver variable which will allow us to \n",
    "    save our trained model for later use'''\n",
    "    \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have specified our model, we are ready to train it. We do this by iteratively calling the model, with each call representing one training step. At each step, we:\n",
    "\n",
    "- Feed in a new set of training data. Remember that with SGD we only have to feed in a small set of data at a time. The size of each batch of training data is determined by the 'batch_size' hyper-parameter specified above.\n",
    "\n",
    "- Call the optimizer function by asking tensorflow to return the model's 'optimizer' variable. This starts a chain reaction in Tensorflow that executes all the computation necessary to train the model. The optimizer function itself will compute the gradients in the model and modify the weight and bias parameters in a way that minimizes the overall loss. Because it needs this loss to compute the gradients, it will also trigger the loss function, which will in turn trigger the model to compute predictions based on the input data. This sort of chain reaction is at the root of the 'lazy evaluation' model used by Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create an array to store the results of the optimization at each epoch\n",
    "results = []\n",
    "\n",
    "'''First we open a session of Tensorflow using our graph as the base. \n",
    "While this session is active all the parameter values will be stored, \n",
    "and each step of training will be using the same model.'''\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    '''After we start a new session we first need to\n",
    "    initialize the values of all the variables.'''\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "\n",
    "    '''Now we iterate through each training epoch based on the hyper-parameter set above.\n",
    "    Each epoch represents a single pass through all the training data.\n",
    "    The total number of training steps is determined by the number of epochs and \n",
    "    the size of mini-batches relative to the size of the entire training set.'''\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        '''At the beginning of each epoch, we create a set of shuffled indexes \n",
    "        so that we are using the training data in a different order each time'''\n",
    "        indexes = range(num_samples)\n",
    "        random.shuffle(indexes)\n",
    "        \n",
    "        '''Next we step through each mini-batch in the training set'''\n",
    "        for step in range(int(math.floor(num_samples/float(batch_size)))):\n",
    "            offset = step * batch_size\n",
    "            \n",
    "            '''We subset the feature and target training sets to create each mini-batch'''\n",
    "            batch_data = X_train[indexes[offset:(offset + batch_size)]]\n",
    "            batch_labels = y_train[indexes[offset:(offset + batch_size)]]\n",
    "\n",
    "            '''Then, we create a 'feed dictionary' that will feed this data, \n",
    "            along with any other hyper-parameters such as the dropout probability,\n",
    "            to the model'''\n",
    "            feed_dict = {x : batch_data, _y : batch_labels, keep_prob: dropout_keep_prob}\n",
    "            \n",
    "            '''Finally, we call the session's run() function, which will feed in \n",
    "            the current training data, and execute portions of the graph as necessary \n",
    "            to return the data we ask for.\n",
    "            \n",
    "            The first argument of the run() function is a list specifying the \n",
    "            model variables we want it to compute and return from the function. \n",
    "            The most important is 'optimizer' which triggers all calculations necessary \n",
    "            to perform one training step. We also include 'loss' and 'prediction' \n",
    "            because we want these as ouputs from the function so we can keep \n",
    "            track of the training process.\n",
    "            \n",
    "            The second argument specifies the feed dictionary that contains \n",
    "            all the data we want to pass into the model at each training step.'''\n",
    "            _, l, p = session.run([optimizer, loss, prediction], feed_dict=feed_dict)\n",
    "\n",
    "        '''At the end of each epoch, we will calcule the error of predictions \n",
    "        on the full training and test data set. We will then store the epoch number, \n",
    "        along with the mini-batch, training, and test accuracies to the 'results' array \n",
    "        so we can visualize the training process later. How often we save the data to \n",
    "        this array is specified by the display_step variable created above''' \n",
    "        if (epoch % display_step == 0):\n",
    "            batch_acc = accuracy(p, batch_labels)\n",
    "            train_acc =  accuracy(train_prediction.eval(session=session), y_train)\n",
    "            test_acc =  accuracy(test_prediction.eval(session=session), y_test)\n",
    "            results.append([epoch, batch_acc, train_acc, test_acc])\n",
    "\n",
    "    '''Once training is complete, we will save the trained model so that we can use it later'''\n",
    "    save_path = saver.save(session, \"model_houses.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now that the model is trained, let's visualize the training process by plotting the error we achieved in the small training batch, the full training set, and the test set at each epoch. We will also print out the minimum loss we were able to achieve in the test set over all the training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=results, columns = [\"epoch\", \"batch_acc\", \"train_acc\", \"test_acc\"])\n",
    "df.set_index(\"epoch\", drop=True, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    " \n",
    "ax.plot(df)\n",
    "ax.set(xlabel='Epoch',\n",
    "       ylabel='Error',\n",
    "       title='Training result')\n",
    " \n",
    "ax.legend(df.columns, loc=1)\n",
    "\n",
    "print \"Minimum test loss:\", np.min(df[\"test_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "From the plot you can see several things:\n",
    "\n",
    "- the error on the training data smoothly improves throughout the training, which is to be expected from the gradient descent algorithm\n",
    "- the error of each mini-batch is more noisy than the entire training set (which is also to be expected since we are only using a portion of the data each time) but in general follows the same trajectory\n",
    "- the error over the training set bottoms out around the 120th epoch, which might represent the best model fit for this dataset\n",
    "\n",
    "All of this is to be expected, however the most important thing to notice with this plot is the error over the test set (which is actually measuring the generalized performance of the model). You can see that for the first 35 or so epochs the error over the test set improves in pace with the error over the training set. This means that the tuning of the model is fitting the underlying structures in both datasets. However, after the 70th epoch, the error over the test starts to move back up. This is a very common indication that overfitting of the training set has occured. After this point, further tuning of the model is representing particular features of the training set itself (such as it's particular error or noise), which do not generalize well to other data not seen by the training process. This shows why it is so important to use a separate testing set to evaluate a model, since otherwise it would be impossible to see exactly where this point of overfitting occurs.\n",
    "\n",
    "### Assignment - part 1\n",
    "\n",
    "There are several common strategies for addressing overfitting which we will cover in class and are also covered [here](http://neuralnetworksanddeeplearning.com/chap3.html). Go back to the neural network and experiment with different settings for some of the hyper-parameters to see if you can fix this 'double-dip' in the test set error. One approach might be to reduce the number of layers in the network or the number of neurons in each layer, since a simpler model is less likely to overfit. Another approach might be to increase the amount of dropout, since this will artificially limit the complexity of the model.\n",
    "\n",
    "_Bonus:_ there is one fundamental issue with how I'm using the data from the very beginning which is also contributing to the overfitting problem in this particular case. Can you think of something we can do to the data before training which would ensure that the training and test sets are more similar?\n",
    "\n",
    "Once you fix the overfitting problem and achieve a minimum test loss of less than 6.0, submit your work as a pull request back to the main project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
